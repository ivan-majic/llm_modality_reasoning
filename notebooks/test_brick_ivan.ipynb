{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "import json\n",
    "import argparse\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"test LLM planning abilities\")\n",
    "parser.add_argument(\n",
    "        \"--dim\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"dimension of the problem 1 or 2 (for 1D or 2D)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "        \"--prompt_type\",\n",
    "        type=str,\n",
    "        default='text',\n",
    "        help=\"type of prompt\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--color\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Use color images (1) or not (0)\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "        default='all',\n",
    "        help=\"num\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--key\",\n",
    "        type=str,\n",
    "        default='',\n",
    "        help=\"num\",\n",
    "    )\n",
    "\n",
    "args_string = '--dim=2 --prompt_type=image --color=1'\n",
    "args_list = args_string.split(' ')\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model_name=='all':\n",
    "    model_name = [\n",
    "        'gpt-3.5-turbo',\n",
    "        'gpt-4o-mini',\n",
    "        'gpt-4o-2024-08-06'\n",
    "    ]\n",
    "else:\n",
    "    model_name = args.model_name\n",
    "# model_name='gpt-3.5-turbo'\n",
    "# model_name='gpt-4o-mini'\n",
    "model_name='gpt-4o-2024-08-06'\n",
    "if not args.key:\n",
    "    apikey_filepath = '../.openai_key.txt'\n",
    "    with open(apikey_filepath, 'r') as f:\n",
    "        key = f.read()\n",
    "else:\n",
    "    key = args.key\n",
    "\n",
    "openai_client = ChatOpenAI(api_key=key, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "if args.dim == 1:\n",
    "    examples_data_dir = '../data/brick_1D_50'\n",
    "elif args.dim == 2:\n",
    "    examples_data_dir = '../data/brick_2D_50'\n",
    "\n",
    "with open(os.path.join(examples_data_dir, 'data.json')) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "zeroshot_prompt = 'Lets think step by step, and provide the answer in the format of a sequence of bricks by a comma in the last sentence.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.prompt_type.lower() in ['text', 'txt']:\n",
    "    res_list = []\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        print('step: ', i+1)\n",
    "        response = openai_client(messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": zeroshot_prompt + '\\n\\n' + 'Question:' + '\\n' + item['data'] + '\\nAnswer:\\n'}\n",
    "        ],\n",
    "        max_tokens=2048,\n",
    "        temperature=0)\n",
    "        res = response.content\n",
    "\n",
    "        dict_res = {'pred': res, 'label': item['label']}\n",
    "        res_list.append(dict_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1\n",
      "step:  2\n",
      "step:  3\n",
      "step:  4\n",
      "step:  5\n",
      "step:  6\n",
      "step:  7\n",
      "step:  8\n",
      "step:  9\n",
      "step:  10\n",
      "step:  11\n",
      "step:  12\n",
      "step:  13\n",
      "step:  14\n",
      "step:  15\n",
      "step:  16\n",
      "step:  17\n",
      "step:  18\n",
      "step:  19\n",
      "step:  20\n",
      "step:  21\n",
      "step:  22\n",
      "step:  23\n",
      "step:  24\n",
      "step:  25\n",
      "step:  26\n",
      "step:  27\n",
      "step:  28\n",
      "step:  29\n",
      "step:  30\n",
      "step:  31\n",
      "step:  32\n",
      "step:  33\n",
      "step:  34\n",
      "step:  35\n",
      "step:  36\n",
      "step:  37\n",
      "step:  38\n",
      "step:  39\n",
      "step:  40\n",
      "step:  41\n",
      "step:  42\n",
      "step:  43\n",
      "step:  44\n",
      "step:  45\n",
      "step:  46\n",
      "step:  47\n",
      "step:  48\n",
      "step:  49\n",
      "step:  50\n"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['image','img']:\n",
    "\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "    image_subdir = 'images'\n",
    "    if args.color:\n",
    "        color='color'\n",
    "    else:\n",
    "        color='bw'\n",
    "\n",
    "    img_prompt = 'The image shows a set of bricks that can be placed on top of each other. Now we have to get a specific brick. The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first. How to get brick {t}?'\n",
    "    res_list = []\n",
    "    i = 0\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        print('step: ', i+1)\n",
    "        image_path = os.path.join(examples_data_dir, image_subdir, f'img_{i}_{color}.png')\n",
    "        image_base64 = encode_image(image_path)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", zeroshot_prompt + img_prompt.format(t=item['target'])),\n",
    "                (\n",
    "                    \"user\",\n",
    "                    [\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                        }\n",
    "                    ],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | openai_client\n",
    "        res = chain.invoke({'image_data':image_base64}).content\n",
    "        dict_res = {'pred': res, 'label': item['label']}\n",
    "        res_list.append(dict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../data/results/2D_image_color_gpt-4o-2024-08-06.json\n"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['img','image']:\n",
    "    colorname = '_color' if args.color else '_bw'\n",
    "else:\n",
    "    colorname = ''\n",
    "\n",
    "output_path = os.path.join('../data/results/', f'{args.dim}D_{args.prompt_type}{colorname}_{model_name}.json')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(res_list, outfile)\n",
    "\n",
    "print(f'Results saved to {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sigspatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
