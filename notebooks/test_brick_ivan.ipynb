{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:07.411514308Z",
     "start_time": "2024-08-29T18:45:07.053059652Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "import json\n",
    "import argparse\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:08.153862136Z",
     "start_time": "2024-08-29T18:45:08.145688526Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"test LLM planning abilities\")\n",
    "parser.add_argument(\n",
    "        \"--dim\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"dimension of the problem 1 or 2 (for 1D or 2D)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "        \"--input_prompt_type\",\n",
    "        type=str,\n",
    "        default='text',\n",
    "        help=\"type of prompt: text, image\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--color\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Use color images (1) or not (0)\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "        default='all',\n",
    "        help=\"num\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--openai_key\",\n",
    "        type=str,\n",
    "        default='',\n",
    "        help=\"num\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--gemini_key\",\n",
    "        type=str,\n",
    "        default='',\n",
    "        help=\"num\",\n",
    "    )\n",
    "\n",
    "args_string = '--dim=1 --prompt_type=both --color=1'\n",
    "args_list = args_string.split(' ')\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:14.226677820Z",
     "start_time": "2024-08-29T18:45:14.168546229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt model\n"
     ]
    }
   ],
   "source": [
    "if args.model_name=='all':\n",
    "    model_name = [\n",
    "        'gpt-3.5-turbo',\n",
    "        'gpt-4o-mini',\n",
    "        'gpt-4o-2024-08-06'\n",
    "    ]\n",
    "else:\n",
    "    model_name = args.model_name\n",
    "# model_name='gpt-3.5-turbo'\n",
    "# model_name='gpt-4o-mini'\n",
    "model_name='gpt-4o-2024-08-06'\n",
    "# model_name='gemini-1.5-pro'\n",
    "\n",
    "if not args.openai_key:\n",
    "    apikey_filepath = '../.openai_key.txt'\n",
    "    with open(apikey_filepath, 'r') as f:\n",
    "        openai_key = f.read()\n",
    "else:\n",
    "    openai_key = args.key\n",
    "\n",
    "if not args.gemini_key:\n",
    "    gemini_key_filepath = '../.gemini_api_key.txt'\n",
    "    with open(gemini_key_filepath, 'r') as f:\n",
    "        gemini_key = f.read()\n",
    "else:\n",
    "    gemini_key = args.gemini_key\n",
    "\n",
    "if 'gpt' in model_name:\n",
    "    print('gpt model')\n",
    "    client = ChatOpenAI(api_key=openai_key, model_name=model_name)\n",
    "elif 'gemini' in model_name:\n",
    "    print('gemini model')\n",
    "    client = ChatGoogleGenerativeAI(google_api_key=gemini_key, model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:21.522969788Z",
     "start_time": "2024-08-29T18:45:21.516510305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "if args.dim == 1:\n",
    "    examples_data_dir = '../data/brick_1D_50'\n",
    "elif args.dim == 2:\n",
    "    examples_data_dir = '../data/brick_2D_50'\n",
    "\n",
    "with open(os.path.join(examples_data_dir, 'data.json')) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "zeroshot_prompt = 'Lets think step by step, and provide the answer in the format of a sequence of bricks by a comma in the last sentence.'\n",
    "zeroshot_prompt = 'Lets think step by step, and provide the answer in the format of a sequence of bricks by a comma in the last sentence.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.prompt_type.lower() in ['text', 'txt']:\n",
    "    res_list = []\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        print('step: ', i+1)\n",
    "        response = client(messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": zeroshot_prompt + '\\n\\n' + 'Question:' + '\\n' + item['data'] + '\\nAnswer:\\n'}\n",
    "        ],\n",
    "        max_tokens=2048,\n",
    "        temperature=0)\n",
    "        res = response.content\n",
    "\n",
    "        dict_res = {'pred': res, 'label': item['label']}\n",
    "        res_list.append(dict_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:47:26.736972868Z",
     "start_time": "2024-08-29T18:47:26.075222388Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.prompt_type.lower() in ['image','img']:\n",
    "\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "    image_subdir = 'images'\n",
    "    if args.color:\n",
    "        color='color'\n",
    "    else:\n",
    "        color='bw'\n",
    "\n",
    "    img_prompt = 'The image shows a set of bricks that can be placed on top of each other. Now we have to get a specific brick. The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first. How to get brick {t}?'\n",
    "    res_list = []\n",
    "    i = 0\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        print('step: ', i+1)\n",
    "        image_path = os.path.join(examples_data_dir, image_subdir, f'img_{i}_{color}.png')\n",
    "        image_base64 = encode_image(image_path)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", zeroshot_prompt + img_prompt.format(t=item['target'])),\n",
    "                (\n",
    "                    \"user\",\n",
    "                    [\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                        }\n",
    "                    ],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | client\n",
    "        res = chain.invoke({'image_data':image_base64}).content\n",
    "        dict_res = {'pred': res, 'label': item['label']}\n",
    "        res_list.append(dict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1\n",
      "step:  2\n",
      "step:  3\n",
      "step:  4\n",
      "step:  5\n",
      "step:  6\n",
      "step:  7\n",
      "step:  8\n",
      "step:  9\n",
      "step:  10\n",
      "step:  11\n",
      "step:  12\n",
      "step:  13\n",
      "step:  14\n",
      "step:  15\n",
      "step:  16\n",
      "step:  17\n",
      "step:  18\n",
      "step:  19\n",
      "step:  20\n",
      "step:  21\n",
      "step:  22\n",
      "step:  23\n",
      "step:  24\n",
      "step:  25\n",
      "step:  26\n",
      "step:  27\n",
      "step:  28\n",
      "step:  29\n",
      "step:  30\n",
      "step:  31\n",
      "step:  32\n",
      "step:  33\n",
      "step:  34\n",
      "step:  35\n",
      "step:  36\n",
      "step:  37\n",
      "step:  38\n",
      "step:  39\n",
      "step:  40\n",
      "step:  41\n",
      "step:  42\n",
      "step:  43\n",
      "step:  44\n",
      "step:  45\n",
      "step:  46\n",
      "step:  47\n",
      "step:  48\n",
      "step:  49\n",
      "step:  50\n"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['both','all','text+img']:\n",
    "\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "    image_subdir = 'images'\n",
    "    if args.color:\n",
    "        color='color'\n",
    "    else:\n",
    "        color='bw'\n",
    "\n",
    "    txt_img_prompt_suffix = ' The described brick layout is visualized in the attached image. You can use both image and textual description to solve the task.'\n",
    "    res_list = []\n",
    "    i = 0\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        print('step: ', i+1)\n",
    "        image_path = os.path.join(examples_data_dir, image_subdir, f'img_{i}_{color}.png')\n",
    "        image_base64 = encode_image(image_path)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", zeroshot_prompt),\n",
    "                (\n",
    "                    \"user\",\n",
    "                    [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": item['data'] + txt_img_prompt_suffix\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                        }\n",
    "                    ],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | client\n",
    "        res = chain.invoke({'image_data':image_base64}).content\n",
    "        dict_res = {'pred': res, 'label': item['label']}\n",
    "        res_list.append(dict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../data/results/1D_both_color_gpt-4o-2024-08-06.json\n"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['img','image','both','all','text+img']:\n",
    "    colorname = '_color' if args.color else '_bw'\n",
    "else:\n",
    "    colorname = ''\n",
    "\n",
    "output_path = os.path.join('../data/results/', f'{args.dim}D_{args.prompt_type}{colorname}_{model_name}.json')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(res_list, outfile)\n",
    "\n",
    "print(f'Results saved to {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sigspatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
