{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:07.411514308Z",
     "start_time": "2024-08-29T18:45:07.053059652Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "import json\n",
    "import argparse\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:08.153862136Z",
     "start_time": "2024-08-29T18:45:08.145688526Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"test LLM planning abilities\")\n",
    "parser.add_argument(\n",
    "        \"--dim\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"dimension of the problem 1 or 2 (for 1D or 2D)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "        \"--prompt_type\",\n",
    "        type=str,\n",
    "        default='text',\n",
    "        help=\"type of prompt: text, image, or both\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--color\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Use color images (1) or not (0)\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "        default='all',\n",
    "        help=\"num\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--openai_key\",\n",
    "        type=str,\n",
    "        default='',\n",
    "        help=\"num\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--gemini_key\",\n",
    "        type=str,\n",
    "        default='',\n",
    "        help=\"num\",\n",
    "    )\n",
    "\n",
    "args_string = '--dim=1 --prompt_type=text --color=0'\n",
    "args_list = args_string.split(' ')\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:14.226677820Z",
     "start_time": "2024-08-29T18:45:14.168546229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt model\n"
     ]
    }
   ],
   "source": [
    "if args.model_name=='all':\n",
    "    model_name = [\n",
    "        'gpt-3.5-turbo',\n",
    "        'gpt-4o-mini',\n",
    "        'gpt-4o-2024-08-06'\n",
    "    ]\n",
    "else:\n",
    "    model_name = args.model_name\n",
    "# model_name='gpt-3.5-turbo'\n",
    "model_name='gpt-4o-mini'\n",
    "# model_name='gpt-4o-2024-08-06'\n",
    "# model_name='gemini-1.5-pro'\n",
    "\n",
    "if not args.openai_key:\n",
    "    apikey_filepath = '../.openai_key.txt'\n",
    "    with open(apikey_filepath, 'r') as f:\n",
    "        openai_key = f.read()\n",
    "else:\n",
    "    openai_key = args.key\n",
    "\n",
    "if not args.gemini_key:\n",
    "    gemini_key_filepath = '../.gemini_api_key.txt'\n",
    "    with open(gemini_key_filepath, 'r') as f:\n",
    "        gemini_key = f.read()\n",
    "else:\n",
    "    gemini_key = args.gemini_key\n",
    "\n",
    "if 'gpt' in model_name:\n",
    "    print('gpt model')\n",
    "    client = OpenAI(api_key=openai_key, model_name=model_name)\n",
    "elif 'gemini' in model_name:\n",
    "    print('gemini model')\n",
    "    client = ChatGoogleGenerativeAI(google_api_key=gemini_key, model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:45:21.522969788Z",
     "start_time": "2024-08-29T18:45:21.516510305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "if args.dim == 1:\n",
    "    examples_data_dir = '../data/brick_1D_50'\n",
    "elif args.dim == 2:\n",
    "    examples_data_dir = '../data/brick_2D_50'\n",
    "\n",
    "with open(os.path.join(examples_data_dir, 'data.json')) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "zeroshot_prompt = 'Lets think step by step, and provide the answer in the format of a sequence of bricks by a comma in the last sentence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m text_prompt_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGiven the textual description below, generate an image that explains to another instance of AI what this text describes, so that you can both answer spatial reasoning questions about it in the same way.\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      8\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m     template\u001b[38;5;241m=\u001b[39m text_prompt_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{image_desc}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([data[\u001b[38;5;241m0\u001b[39m]]):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;124m'\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/pcloud/01_Projects/2024_SIGSPATIAL/llm_modality_reasoning/venv_sigspatial/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:215\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     emit_warning()\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pcloud/01_Projects/2024_SIGSPATIAL/llm_modality_reasoning/venv_sigspatial/lib/python3.11/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pcloud/01_Projects/2024_SIGSPATIAL/llm_modality_reasoning/venv_sigspatial/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['text', 'txt']:\n",
    "    from openai import OpenAI\n",
    "    # generate image based on the text prompt\n",
    "    res_list = []\n",
    "\n",
    "    text_prompt_prefix = 'Given the textual description below, generate an image that explains to another instance of AI what this text describes, so that you can both answer spatial reasoning questions about it in the same way.' \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"image_desc\"],\n",
    "        template= text_prompt_prefix + \"\\n\\n {image_desc}\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    for i, item in enumerate([data[0]]):\n",
    "        print('step: ', i+1)\n",
    "\n",
    "        client = OpenAI(api_key=openai_key)\n",
    "\n",
    "        response = client.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=text_prompt_prefix + \"\\n\\n {item['data']}\",\n",
    "            size=\"1024x1024\",\n",
    "            quality=\"standard\",\n",
    "            n=1,\n",
    "            )\n",
    "\n",
    "        image_url = response.data[0].url\n",
    "\n",
    "        print(image_url)\n",
    "        # res = response.content\n",
    "\n",
    "        # dict_res = {'pred': res, 'label': item['label']}\n",
    "        # res_list.append(dict_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the textual description below, generate an image that explains to another instance of AI what this text describes, so that you can both answer spatial reasoning questions about it in the same way.\\n\\n There is a set of bricks. The brick V is on top of the brick N . The brick S is on top of the brick V . The brick F is on top of the brick E . For the brick J, the color is blue. The brick H is on top of the brick S . The brick R is on top of the brick M . The brick M is on top of the brick I . The brick I is on top of the brick H . The brick N is on top of the brick F . The brick E is on top of the brick J . Now we have to get a specific brick. The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first. How to get brick J?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prompt_prefix + f\"\\n\\n {item['data']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m text_prompt_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGiven the textual description below, generate an image that explains to another instance of AI what this text describes, so that you can both answer spatial reasoning questions about it in the same way.\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      8\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m     template\u001b[38;5;241m=\u001b[39m text_prompt_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{image_desc}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([data[\u001b[38;5;241m0\u001b[39m]]):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;124m'\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/pcloud/01_Projects/2024_SIGSPATIAL/llm_modality_reasoning/venv_sigspatial/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:215\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     emit_warning()\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pcloud/01_Projects/2024_SIGSPATIAL/llm_modality_reasoning/venv_sigspatial/lib/python3.11/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pcloud/01_Projects/2024_SIGSPATIAL/llm_modality_reasoning/venv_sigspatial/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['text', 'txt']:\n",
    "    # generate image based on the text prompt\n",
    "    res_list = []\n",
    "    llm = OpenAI(api_key=openai_key)\n",
    "\n",
    "    text_prompt_prefix = 'Given the textual description below, generate an image that explains to another instance of AI what this text describes, so that you can both answer spatial reasoning questions about it in the same way.' \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"image_desc\"],\n",
    "        template= text_prompt_prefix + \"\\n\\n {image_desc}\",\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    for i, item in enumerate([data[0]]):\n",
    "        print('step: ', i+1)\n",
    "\n",
    "        image_url = DallEAPIWrapper(api_key=openai_key).run(chain.run(item['data']))\n",
    "        print(image_url)\n",
    "        # res = response.content\n",
    "\n",
    "        # dict_res = {'pred': res, 'label': item['label']}\n",
    "        # res_list.append(dict_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pred': 'To visualize the arrangement of the bricks based on the description provided, we can create a diagram that shows the stacking of the bricks. Here’s how the bricks are arranged according to the description:\\n\\n1. **Top Layer**:\\n   - H (on top of S)\\n   - S (on top of V)\\n   - V (on top of N)\\n\\n2. **Middle Layer**:\\n   - N (on top of F)\\n   - F (on top of E)\\n   - E (on top of J)\\n\\n3. **Bottom Layer**:\\n   - R (on top of M)\\n   - M (on top of I)\\n   - I (on top of H)\\n\\nNow, to get brick J, we need to follow the order of removal from top to bottom:\\n\\n1. Remove brick H (which is on top of I).\\n2. Remove brick I (which is on top of M).\\n3. Remove brick M (which is on top of R).\\n4. Remove brick R.\\n5. Remove brick N (which is on top of F).\\n6. Remove brick F (which is on top of E).\\n7. Finally, remove brick E (which is on top of J).\\n8. Now, brick J can be taken.\\n\\nThis sequence of removals ensures that we can access brick J by following the stacking order. \\n\\nThe image should depict the bricks in their respective positions, clearly showing which bricks are on top of others, and highlight the path to remove bricks to finally access brick J.',\n",
       "  'label': 'RMIHSVNFEJ'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T18:47:26.736972868Z",
     "start_time": "2024-08-29T18:47:26.075222388Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.prompt_type.lower() in ['image','img']:\n",
    "\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "    image_subdir = 'images'\n",
    "    if args.color:\n",
    "        color='color'\n",
    "    else:\n",
    "        color='bw'\n",
    "\n",
    "    image_prompt_prefix = 'Given the image below, explain to another instance of AI what this image shows, so that you can both answer spatial reasoning questions about it in the same way.'\n",
    "    # img_prompt = 'The image shows a set of bricks that can be placed on top of each other. Now we have to get a specific brick. The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first. How to get brick {t}?'\n",
    "    res_list = []\n",
    "    i = 0\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        print('step: ', i+1)\n",
    "        image_path = os.path.join(examples_data_dir, image_subdir, f'img_{i}_{color}.png')\n",
    "        image_base64 = encode_image(image_path)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", image_prompt_prefix),\n",
    "                (\n",
    "                    \"user\",\n",
    "                    [\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                        }\n",
    "                    ],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | client\n",
    "        res = chain.invoke({'image_data':image_base64}).content\n",
    "        dict_res = {'pred': res, 'label': item['label']}\n",
    "        res_list.append(dict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1\n",
      "step:  2\n",
      "step:  3\n",
      "step:  4\n",
      "step:  5\n",
      "step:  6\n",
      "step:  7\n",
      "step:  8\n",
      "step:  9\n",
      "step:  10\n",
      "step:  11\n",
      "step:  12\n",
      "step:  13\n",
      "step:  14\n",
      "step:  15\n",
      "step:  16\n",
      "step:  17\n",
      "step:  18\n",
      "step:  19\n",
      "step:  20\n",
      "step:  21\n",
      "step:  22\n",
      "step:  23\n",
      "step:  24\n",
      "step:  25\n",
      "step:  26\n",
      "step:  27\n",
      "step:  28\n",
      "step:  29\n",
      "step:  30\n",
      "step:  31\n",
      "step:  32\n",
      "step:  33\n",
      "step:  34\n",
      "step:  35\n",
      "step:  36\n",
      "step:  37\n",
      "step:  38\n",
      "step:  39\n",
      "step:  40\n",
      "step:  41\n",
      "step:  42\n",
      "step:  43\n",
      "step:  44\n",
      "step:  45\n",
      "step:  46\n",
      "step:  47\n",
      "step:  48\n",
      "step:  49\n",
      "step:  50\n"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['both','all','text+img']:\n",
    "\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "    image_subdir = 'images'\n",
    "    if args.color:\n",
    "        color='color'\n",
    "    else:\n",
    "        color='bw'\n",
    "\n",
    "    txt_img_prompt_suffix = ' The described brick layout is visualized in the attached image. You can use both image and textual description to solve the task.'\n",
    "    res_list = []\n",
    "    i = 0\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        print('step: ', i+1)\n",
    "        image_path = os.path.join(examples_data_dir, image_subdir, f'img_{i}_{color}.png')\n",
    "        image_base64 = encode_image(image_path)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", zeroshot_prompt),\n",
    "                (\n",
    "                    \"user\",\n",
    "                    [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": item['data'] + txt_img_prompt_suffix\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                        }\n",
    "                    ],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | client\n",
    "        res = chain.invoke({'image_data':image_base64}).content\n",
    "        dict_res = {'pred': res, 'label': item['label']}\n",
    "        res_list.append(dict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../data/results/1D_both_color_gpt-4o-2024-08-06.json\n"
     ]
    }
   ],
   "source": [
    "if args.prompt_type.lower() in ['img','image','both','all','text+img']:\n",
    "    colorname = '_color' if args.color else '_bw'\n",
    "else:\n",
    "    colorname = ''\n",
    "\n",
    "output_path = os.path.join('../data/results/', f'{args.dim}D_{args.prompt_type}{colorname}_{model_name}.json')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(res_list, outfile)\n",
    "\n",
    "print(f'Results saved to {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sigspatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
